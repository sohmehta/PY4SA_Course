{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Python for Spatial Analysis\n",
    "## Second part of the module of GG3209 Spatial Analysis with GIS.\n",
    "### Notebook to learn and practice Pandas\n",
    "\n",
    "---\n",
    "Dr Fernando Benitez -  University of St Andrews - School of Geography and Sustainable Development - First Iteration 2023 v.1.0 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction \n",
    "\n",
    "After practicing **NumPy**, this notebook aims to work with the library **Pandas** which allows for reading in and working with data tables.\n",
    "\n",
    "Most geo spatial scientists are first introduced to data tables in the form of an Excel Spreadsheet. In such a structure, each record or feature is represented by a row of data while each column represents a specific piece of information for each record. Sometimes we call that a variable, or attribute. \n",
    "\n",
    "Further, spreadsheets are able to hold different data types in each column. A comparable data structure would be handy for use in Python. This is made available by the **Pandas** library. Pandas allows for data to be stored in **DataFrames**. If you work in the R environment, this is very similar to the concept of data frames in R. In fact, *Pandas DataFrames were inspired by R data frames*. \n",
    "\n",
    "Pandas makes use of the NumPy library, so it is generally a good idea to import NumPy if you plan to use Pandas. Also, you will need to install Pandas and NumPy into your environment prior to using them., but we have include both libraries in our python environment py4sa.yml\n",
    "\n",
    "The complete documentation for Pandas can be found [here](https://pandas.pydata.org/).\n",
    "\n",
    "After working through this module you will be able to:\n",
    "\n",
    "1. Create and manipulate **Series** and **DataFrames** using Pandas.\n",
    "2. Query and subset DataFrames.\n",
    "3. Manipulate DataFrames.\n",
    "4. Summarize and group DataFrames.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Intro to Series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we talk about DataFrames, I will introduce the concept of a **Series**. These are actually very similar to a NumPy array except that they allow for axis labels to be assigned. Examples of generating a series from **lists**, **NumPy arrays**, and **dictionaries** are provided below. \n",
    "\n",
    "A series is comparable to a single column from a Spreadsheet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class1                    GIS\n",
      "Class2         Remote Sensing\n",
      "Class3       Spatial Analysis\n",
      "Class4    Digital Cartography\n",
      "dtype: object\n",
      "Class1    350\n",
      "Class2    455\n",
      "Class3    457\n",
      "Class4    642\n",
      "dtype: int64\n",
      "Class1                    GIS\n",
      "Class2         Remote Sensing\n",
      "Class3       Spatial Analysis\n",
      "Class4    Digital Cartography\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "lst1 = [\"GIS\", \"Remote Sensing\", \"Spatial Analysis\", \"Digital Cartography\"]\n",
    "arr1 = np.array([350, 455, 457, 642])\n",
    "dict1 = {'Class1':\"GIS\", \"Class2\":\"Remote Sensing\", \"Class3\":\"Spatial Analysis\", \"Class4\":\"Digital Cartography\"}\n",
    "\n",
    "s_lst = pd.Series(data=lst1, index = [\"Class1\", \"Class2\", \"Class3\", \"Class4\"])\n",
    "s_arr = pd.Series(data=arr1, index = [\"Class1\", \"Class2\", \"Class3\", \"Class4\"])\n",
    "s_dict = pd.Series(dict1)\n",
    "\n",
    "print(s_lst)\n",
    "print(s_arr)\n",
    "print(s_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Labels or names can then be used to select data either using **bracket notation** or **dot notation**. \n",
    "\n",
    "You can use whichever method you prefer. However, if you use dot notation you should not included spaces in the column names. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spatial Analysis\n",
      "Spatial Analysis\n"
     ]
    }
   ],
   "source": [
    "print(s_dict[\"Class3\"])\n",
    "print(s_dict.Class3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intro to DataFrames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start by building a DataFrame from a set of lists. First, you can create three lists to hold different components of course title information.\n",
    "\n",
    "Next, you could combine these lists into a dictionary. Finally, you can convert the dictionary into a DataFrame. \n",
    "\n",
    "Note that a well formatted table is generated by just calling the DataFrame name without the *print()* function; however, in the example we use *print()* (but try out both ways to see the difference). Also, the **keys** from the dictionary have been used as the column names, and a default index has been assigned to each row.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  prefix  course_number          course_name\n",
      "0   Geol            103   Earth Through Time\n",
      "1   Geol            321        Geomorphology\n",
      "2   Geol            331         Paleontology\n",
      "3   Geol            341   Structural Geology\n",
      "4   Geog            350            GIScience\n",
      "5   Geog            455       Remote Sensing\n",
      "6   Geog            462  Digital Cartography\n"
     ]
    }
   ],
   "source": [
    "prefix = [\"Geol\", \"Geol\", \"Geol\", \"Geol\", \"Geog\", \"Geog\", \"Geog\"]\n",
    "cnum = [103, 321, 331, 341, 350, 455, 462]\n",
    "cname = [\"Earth Through Time\", \"Geomorphology\", \"Paleontology\", \"Structural Geology\", \"GIScience\", \"Remote Sensing\", \"Digital Cartography\"]\n",
    "course_dict = {\"prefix\": prefix, \"course_number\": cnum, \"course_name\": cname}\n",
    "course_df = pd.DataFrame(course_dict)\n",
    "#course_df\n",
    "print(course_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since column names are assigned, they can be used to select out individual columns using bracket or dot notation. Single columns can be saved as a Series. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0     Earth Through Time\n",
      "1          Geomorphology\n",
      "2           Paleontology\n",
      "3     Structural Geology\n",
      "4              GIScience\n",
      "5         Remote Sensing\n",
      "6    Digital Cartography\n",
      "Name: course_name, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(course_df[\"course_name\"])\n",
    "# Or course_df.course_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A list of column names can be provided to subset out multiple columns. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   course_number          course_name\n",
      "0            103   Earth Through Time\n",
      "1            321        Geomorphology\n",
      "2            331         Paleontology\n",
      "3            341   Structural Geology\n",
      "4            350            GIScience\n",
      "5            455       Remote Sensing\n",
      "6            462  Digital Cartography\n"
     ]
    }
   ],
   "source": [
    "print(course_df[[\"course_number\", \"course_name\"]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Indexing with ```.loc``` and ```.iloc```\n",
    "\n",
    "Pandas created the methods **.loc[]** and **.iloc[]** as more flexible alternatives for accessing data from a dataframe. \n",
    "\n",
    "Use ```df.iloc[]``` for indexing with integers and ```df.loc[]``` for indexing with labels.\n",
    "\n",
    "These are typically the recommended methods of indexing in Pandas\n",
    "\n",
    "1. Using the **.loc** method, you can subset based on **column names and row labels** combined, and \n",
    "\n",
    "2. The **.iloc** method, in contrast, is used for **selection based on indexes**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   course_number    course_name\n",
      "1            321  Geomorphology\n",
      "2            331   Paleontology\n",
      "4            350      GIScience\n"
     ]
    }
   ],
   "source": [
    "print(course_df.loc[[1, 2, 4],[\"course_number\", \"course_name\"]])\n",
    "# Or course_df.iloc[[1, 2, 4],[1, 2]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can even use the data stored in existing columns to create a new column. Note that the new column does not need to be declared prior to writing to it. In the example, I have written the entire course name to a new column. The *map()* method is used to make sure all data are treated as strings. It allow for the same function, in this case **str()**, to be applied to each element in an iterable, in this case each row in the DataFrame. I am including blank spaces so that the components are not ran together. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  prefix  course_number          course_name                      full_name\n",
      "0   Geol            103   Earth Through Time   Geol 103: Earth Through Time\n",
      "1   Geol            321        Geomorphology        Geol 321: Geomorphology\n",
      "2   Geol            331         Paleontology         Geol 331: Paleontology\n",
      "3   Geol            341   Structural Geology   Geol 341: Structural Geology\n",
      "4   Geog            350            GIScience            Geog 350: GIScience\n",
      "5   Geog            455       Remote Sensing       Geog 455: Remote Sensing\n",
      "6   Geog            462  Digital Cartography  Geog 462: Digital Cartography\n"
     ]
    }
   ],
   "source": [
    "course_df[\"full_name\"] = course_df[\"prefix\"].map(str)  + \" \" + course_df[\"course_number\"].map(str)  + \": \" + course_df[\"course_name\"].map(str) \n",
    "print(course_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading external files\n",
    "\n",
    "Instead of creating data tables or DataFrames manually, you are probably more likely to read in a data table from a file (CSV) or web link. \n",
    "\n",
    "Fortunately, **Pandas** provides functions for reading data in from a variety of formats. Here are some examples:\n",
    "\n",
    "* *read_table()*: delimited file (TXT, CSV, etc.)\n",
    "* *read_csv()*: comma-separated values (CSV)\n",
    "* *read_excel()*: Excel Spreadsheet\n",
    "* *read_json()*: JavaScript Object Notation (JSON)\n",
    "* *read_html()*: HTML table\n",
    "* *read_sas()*: SAS file\n",
    "\n",
    "Full documentation on reading in data can be found [here](https://pandas.pydata.org/docs/reference/io.html).\n",
    "\n",
    "In the example below, I am reading in a CSV file from my local computer. The *sep* argument is used to define the deliminator ( like you have done in Excel).\n",
    "\n",
    "However, commas are the default, so *it isn't necessary to include this argument in this case*.\n",
    "\n",
    "Setting the *header* argument to 0 indicated that the first row of data should be treated as column names or headers. \n",
    "\n",
    "It isn't always necessary to specify the character encoding; But a best practice tells that it is necessary due to the use of special characters in some tables. \n",
    "\n",
    "To view the first 10 rows of the data. You can use the *head()* method.\n",
    "\n",
    "The *len()* function returns the number of rows. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>city</th>\n",
       "      <th>country</th>\n",
       "      <th>pop</th>\n",
       "      <th>lat</th>\n",
       "      <th>lon</th>\n",
       "      <th>capital</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>'Abasan al-Jadidah</td>\n",
       "      <td>Palestine</td>\n",
       "      <td>5629</td>\n",
       "      <td>31.31</td>\n",
       "      <td>34.34</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>'Abasan al-Kabirah</td>\n",
       "      <td>Palestine</td>\n",
       "      <td>18999</td>\n",
       "      <td>31.32</td>\n",
       "      <td>34.35</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>'Abdul Hakim</td>\n",
       "      <td>Pakistan</td>\n",
       "      <td>47788</td>\n",
       "      <td>30.55</td>\n",
       "      <td>72.11</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>'Abdullah-as-Salam</td>\n",
       "      <td>Kuwait</td>\n",
       "      <td>21817</td>\n",
       "      <td>29.36</td>\n",
       "      <td>47.98</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>'Abud</td>\n",
       "      <td>Palestine</td>\n",
       "      <td>2456</td>\n",
       "      <td>32.03</td>\n",
       "      <td>35.07</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>'Abwein</td>\n",
       "      <td>Palestine</td>\n",
       "      <td>3434</td>\n",
       "      <td>32.03</td>\n",
       "      <td>35.20</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>'Adadlay</td>\n",
       "      <td>Somalia</td>\n",
       "      <td>9198</td>\n",
       "      <td>9.77</td>\n",
       "      <td>44.65</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>'Adale</td>\n",
       "      <td>Somalia</td>\n",
       "      <td>5492</td>\n",
       "      <td>2.75</td>\n",
       "      <td>46.30</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>'Afak</td>\n",
       "      <td>Iraq</td>\n",
       "      <td>22706</td>\n",
       "      <td>32.07</td>\n",
       "      <td>45.26</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>'Afif</td>\n",
       "      <td>Saudi Arabia</td>\n",
       "      <td>41731</td>\n",
       "      <td>23.92</td>\n",
       "      <td>42.93</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 city       country    pop    lat    lon  capital\n",
       "0  'Abasan al-Jadidah     Palestine   5629  31.31  34.34        0\n",
       "1  'Abasan al-Kabirah     Palestine  18999  31.32  34.35        0\n",
       "2        'Abdul Hakim      Pakistan  47788  30.55  72.11        0\n",
       "3  'Abdullah-as-Salam        Kuwait  21817  29.36  47.98        0\n",
       "4               'Abud     Palestine   2456  32.03  35.07        0\n",
       "5             'Abwein     Palestine   3434  32.03  35.20        0\n",
       "6            'Adadlay       Somalia   9198   9.77  44.65        0\n",
       "7              'Adale       Somalia   5492   2.75  46.30        0\n",
       "8               'Afak          Iraq  22706  32.07  45.26        0\n",
       "9               'Afif  Saudi Arabia  41731  23.92  42.93        0"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cities_df = pd.read_csv(\"data/world_cities.csv\", sep=\",\", header=0, encoding=\"ISO-8859-1\")\n",
    "cities_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "43645\n"
     ]
    }
   ],
   "source": [
    "print(len(cities_df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataFrame Query and Subset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now use this data table to explore data query and selection methods. \n",
    "\n",
    "In the first example, I am selecting out Italian cities and saving them to a new DataFrame. Note the use of bracket notation. The code in the middle bracket is used to perform the selection. \n",
    "\n",
    "The second example includes a compound query. Note the use of parenthesis within the query. \n",
    "\n",
    "Lastly, it is also possible to subset out only certain columns that meet the query. In the last example, I am subsetting out just the name of the city and population. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                      city country    pop    lat    lon  capital\n",
      "181            Abano Terme   Italy  19167  45.35  11.78        0\n",
      "197          Abbiategrasso   Italy  30515  45.41   8.91        0\n",
      "310                 Acerra   Italy  51394  41.00  14.39        0\n",
      "329           Aci Castello   Italy  17930  37.56  15.13        0\n",
      "330             Aci Catena   Italy  28459  37.60  15.14        0\n",
      "331       Aci Sant'Antonio   Italy  17566  37.60  15.11        0\n",
      "335               Acireale   Italy  53476  37.62  15.17        0\n",
      "344  Acquaviva delle Fonti   Italy  21555  40.89  16.84        0\n",
      "345            Acqui Terme   Italy  20634  44.68   8.46        0\n",
      "346                   Acri   Italy  21484  39.49  16.38        0\n",
      "\n",
      "985\n",
      "\n",
      "          city country      pop    lat    lon  capital\n",
      "12392    Genoa   Italy   599064  44.42   8.93        0\n",
      "23820    Milan   Italy  1316218  45.48   9.19        0\n",
      "25403   Naples   Italy   983614  40.85  14.27        0\n",
      "27926  Palermo   Italy   668275  38.12  13.36        0\n",
      "31558     Rome   Italy  2561181  41.89  12.50        1\n",
      "39214    Turin   Italy   873123  45.08   7.68        0\n",
      "\n",
      "6\n",
      "\n",
      "         city      pop\n",
      "12392   Genoa   599064\n",
      "23820   Milan  1316218\n",
      "25403  Naples   983614\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Example 1\n",
    "just_italy = cities_df[cities_df[\"country\"]==\"Italy\"]\n",
    "print(just_italy.head(10))\n",
    "print('')\n",
    "print(len(just_italy))\n",
    "print('')\n",
    "#Example 2\n",
    "italy_med_cities = cities_df[(cities_df[\"country\"]==\"Italy\") & (cities_df[\"pop\"]>500000)]\n",
    "print(italy_med_cities)\n",
    "print('')\n",
    "print(len(italy_med_cities))\n",
    "print('')\n",
    "#Example 3\n",
    "italy_cities_pop = cities_df[(cities_df[\"country\"]==\"Italy\") & (cities_df[\"pop\"]>500000)][[\"city\", \"pop\"]]\n",
    "print(italy_cities_pop.head(3))\n",
    "print('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another option for performing queries is to use the **query()** method provided by Pandas. When using this method, the query will need to be provided as an expression in string form. Also, spaces in column names can be problematic, so spaces should be removed or replaced with underscores. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                       city country     pop    lat   lon  capital\n",
      "127                A Coruna   Spain  243088  43.33 -8.42        0\n",
      "128               A Estrada   Spain   21997  42.70 -8.50        0\n",
      "129               A Laracha   Spain   10856  43.25 -8.59        0\n",
      "130    A Pobra do Caraminal   Spain    9955  42.61 -8.94        0\n",
      "182         Abanto Zierbena   Spain    9505  43.32 -3.08        0\n",
      "185                  Abaran   Spain   12890  38.20 -1.40        0\n",
      "253                  Abrera   Spain    9966  41.52  1.90        0\n",
      "413                    Adra   Spain   22900  36.76 -3.02        0\n",
      "573       Aguilar de Campoo   Spain    7224  42.80 -4.27        0\n",
      "574  Aguilar de la Frontera   Spain   13502  37.52 -4.65        0\n",
      "\n",
      "864\n",
      "\n",
      "6\n",
      "\n",
      "            city      pop\n",
      "3385   Barcelona  1591485\n",
      "22032     Madrid  3146804\n",
      "22302     Malaga   553916\n",
      "34629    Sevilla   702516\n",
      "39931   Valencia   803438\n",
      "42691   Zaragoza   658186\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Remove spaces in column names using list comprehension\n",
    "cities_df.columns = [column.replace(\" \", \"_\") for column in cities_df.columns]\n",
    "#Example 1\n",
    "just_spain = cities_df.query('country==\"Spain\"')\n",
    "print(just_spain.head(10))\n",
    "print('')\n",
    "print(len(just_spain))\n",
    "print('')\n",
    "#Example 2\n",
    "spanish_cities_df = cities_df.query('country==\"Spain\" and pop > 500000')\n",
    "print(len(spanish_cities_df))\n",
    "print('')\n",
    "#Example 3\n",
    "subset_query = cities_df.query('country==\"Spain\" and pop > 500000')[[\"city\", \"pop\"]]\n",
    "print(subset_query)\n",
    "print('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once a query is complete, you may want to save the result back to a file on your local machine. The code below provides an example for saving out the last subset of data to a CSV file. The Pandas documentation provides examples for saving to other formats. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset_query.to_csv(\"data/subset_data.csv\", sep=\",\", header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Missing Values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The *NULL*, *NoData*, or missing indicator in Python is *NaN*. To begin exploring missing values, let's recode some of the data to *NaN* in the cities data set. In the example below, I am changing the \"Germany\" and \"Palestine\" countries to *NaN*. I am also recoding any population between 10.000 and 50.000 to *NaN*. The *replace()* method is used to change the categories while the *mask()* method is used to recode the rating values. *np.nan* is a NumPy method for defining null values.  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 city       country     pop    lat    lon  capital\n",
      "0  'Abasan al-Jadidah           NaN  5629.0  31.31  34.34        0\n",
      "1  'Abasan al-Kabirah           NaN     NaN  31.32  34.35        0\n",
      "2        'Abdul Hakim      Pakistan     NaN  30.55  72.11        0\n",
      "3  'Abdullah-as-Salam        Kuwait     NaN  29.36  47.98        0\n",
      "4               'Abud           NaN  2456.0  32.03  35.07        0\n",
      "5             'Abwein           NaN  3434.0  32.03  35.20        0\n",
      "6            'Adadlay       Somalia  9198.0   9.77  44.65        0\n",
      "7              'Adale       Somalia  5492.0   2.75  46.30        0\n",
      "8               'Afak          Iraq     NaN  32.07  45.26        0\n",
      "9               'Afif  Saudi Arabia     NaN  23.92  42.93        0\n",
      "\n",
      "43645\n"
     ]
    }
   ],
   "source": [
    "cities_nan = cities_df.copy()\n",
    "cities_nan[\"country\"] = cities_nan[[\"country\"]].replace([\"Germany\", \"Palestine\"], np.nan)\n",
    "cities_nan['pop'].mask(cities_nan['pop'].between(10000, 50000), inplace=True)\n",
    "print(cities_nan.head(10))\n",
    "print('')\n",
    "print(len(cities_nan))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The *dropna()* method can be used to remove rows or columns that contain missing data. If the axis parameter is set to 0, rows with missing values in any column will be removed. If it is set to 1, columns with missing data in any row will be removed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        city               country        pop    lat    lon  capital\n",
      "6   'Adadlay               Somalia     9198.0   9.77  44.65        0\n",
      "7     'Adale               Somalia     5492.0   2.75  46.30        0\n",
      "10    'Afrin                 Syria    51139.0  36.51  36.87        0\n",
      "14    'Ajlun                Jordan     8629.0  32.34  35.74        0\n",
      "15    'Ajman  United Arab Emirates   238605.0  25.42  55.43        0\n",
      "17    'Al'al                Jordan     4526.0  32.63  35.90        0\n",
      "20      'Ali               Bahrain    57024.0  26.15  50.53        0\n",
      "24    'Alula               Somalia     5158.0  11.97  50.76        0\n",
      "25    'Amman                Jordan  1303197.0  31.95  35.93        1\n",
      "48     'Ataq                 Yemen     2982.0  14.55  46.88        0\n",
      "\n",
      "27875\n",
      "\n",
      "                 city    lat    lon  capital\n",
      "0  'Abasan al-Jadidah  31.31  34.34        0\n",
      "1  'Abasan al-Kabirah  31.32  34.35        0\n",
      "2        'Abdul Hakim  30.55  72.11        0\n",
      "3  'Abdullah-as-Salam  29.36  47.98        0\n",
      "4               'Abud  32.03  35.07        0\n",
      "5             'Abwein  32.03  35.20        0\n",
      "6            'Adadlay   9.77  44.65        0\n",
      "7              'Adale   2.75  46.30        0\n",
      "8               'Afak  32.07  45.26        0\n",
      "9               'Afif  23.92  42.93        0\n",
      "\n",
      "43645\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cities_drop = cities_nan.dropna(axis=0)\n",
    "print(cities_drop.head(10))\n",
    "print('')\n",
    "print(len(cities_drop))\n",
    "print('')\n",
    "cities_dropc = cities_nan.dropna(axis=1)\n",
    "print(cities_dropc.head(10))\n",
    "print('')\n",
    "print(len(cities_dropc))\n",
    "print('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The *.fillna()* method can be used to replace NA values with another value or string. In the example, I am changing the missing genres to \"Unknown Country\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 city          country     pop    lat    lon  capital\n",
      "0  'Abasan al-Jadidah  Unknown Country  5629.0  31.31  34.34        0\n",
      "1  'Abasan al-Kabirah  Unknown Country     NaN  31.32  34.35        0\n",
      "2        'Abdul Hakim         Pakistan     NaN  30.55  72.11        0\n",
      "3  'Abdullah-as-Salam           Kuwait     NaN  29.36  47.98        0\n",
      "4               'Abud  Unknown Country  2456.0  32.03  35.07        0\n",
      "5             'Abwein  Unknown Country  3434.0  32.03  35.20        0\n",
      "6            'Adadlay          Somalia  9198.0   9.77  44.65        0\n",
      "7              'Adale          Somalia  5492.0   2.75  46.30        0\n",
      "8               'Afak             Iraq     NaN  32.07  45.26        0\n",
      "9               'Afif     Saudi Arabia     NaN  23.92  42.93        0\n"
     ]
    }
   ],
   "source": [
    "cities_nan[\"country\"] = cities_nan[\"country\"].fillna(value=\"Unknown Country\")\n",
    "print(cities_nan.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is also possible to replace null values with a statistic derived from the available values. In the example, I am replacing the missing population values with the mean of all available in the attribute. Of course this is not possible, here is just an example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 city          country           pop    lat    lon  capital\n",
      "0  'Abasan al-Jadidah  Unknown Country   5629.000000  31.31  34.34        0\n",
      "1  'Abasan al-Kabirah  Unknown Country  76436.042558  31.32  34.35        0\n",
      "2        'Abdul Hakim         Pakistan  76436.042558  30.55  72.11        0\n",
      "3  'Abdullah-as-Salam           Kuwait  76436.042558  29.36  47.98        0\n",
      "4               'Abud  Unknown Country   2456.000000  32.03  35.07        0\n",
      "5             'Abwein  Unknown Country   3434.000000  32.03  35.20        0\n",
      "6            'Adadlay          Somalia   9198.000000   9.77  44.65        0\n",
      "7              'Adale          Somalia   5492.000000   2.75  46.30        0\n",
      "8               'Afak             Iraq  76436.042558  32.07  45.26        0\n",
      "9               'Afif     Saudi Arabia  76436.042558  23.92  42.93        0\n"
     ]
    }
   ],
   "source": [
    "cities_nan[\"pop\"] = cities_nan[\"pop\"].fillna(value=cities_nan[\"pop\"].mean())\n",
    "print(cities_nan.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grouping and Summarizing DataFrames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pandas provides methods for summarizing data as described in the examples below. First, I am creating individual statistics and saving them to variables. I then create a Series from a dictionary of these statistics, convert it to a DataFrame using the *to_frame()* method, then transpose the DataFrame using *transpose()*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>time</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "      <th>depth</th>\n",
       "      <th>mag</th>\n",
       "      <th>magType</th>\n",
       "      <th>nst</th>\n",
       "      <th>gap</th>\n",
       "      <th>dmin</th>\n",
       "      <th>rms</th>\n",
       "      <th>...</th>\n",
       "      <th>updated</th>\n",
       "      <th>place</th>\n",
       "      <th>type</th>\n",
       "      <th>horizontalError</th>\n",
       "      <th>depthError</th>\n",
       "      <th>magError</th>\n",
       "      <th>magNst</th>\n",
       "      <th>status</th>\n",
       "      <th>locationSource</th>\n",
       "      <th>magSource</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2023-02-23T08:01:02.881Z</td>\n",
       "      <td>38.1280</td>\n",
       "      <td>73.2184</td>\n",
       "      <td>10.000</td>\n",
       "      <td>4.70</td>\n",
       "      <td>mb</td>\n",
       "      <td>75.0</td>\n",
       "      <td>55.0</td>\n",
       "      <td>1.7450</td>\n",
       "      <td>0.66</td>\n",
       "      <td>...</td>\n",
       "      <td>2023-02-23T08:35:43.040Z</td>\n",
       "      <td>65 km W of Murghob, Tajikistan</td>\n",
       "      <td>earthquake</td>\n",
       "      <td>4.00</td>\n",
       "      <td>1.907</td>\n",
       "      <td>0.058</td>\n",
       "      <td>91.0</td>\n",
       "      <td>reviewed</td>\n",
       "      <td>us</td>\n",
       "      <td>us</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2023-02-23T06:55:34.020Z</td>\n",
       "      <td>18.7946</td>\n",
       "      <td>-63.9205</td>\n",
       "      <td>10.000</td>\n",
       "      <td>3.67</td>\n",
       "      <td>md</td>\n",
       "      <td>19.0</td>\n",
       "      <td>228.0</td>\n",
       "      <td>1.1264</td>\n",
       "      <td>0.50</td>\n",
       "      <td>...</td>\n",
       "      <td>2023-02-23T08:02:57.172Z</td>\n",
       "      <td>Leeward Islands</td>\n",
       "      <td>earthquake</td>\n",
       "      <td>5.27</td>\n",
       "      <td>4.420</td>\n",
       "      <td>0.130</td>\n",
       "      <td>14.0</td>\n",
       "      <td>reviewed</td>\n",
       "      <td>pr</td>\n",
       "      <td>pr</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2023-02-23T06:50:49.137Z</td>\n",
       "      <td>38.4879</td>\n",
       "      <td>72.8122</td>\n",
       "      <td>10.000</td>\n",
       "      <td>4.50</td>\n",
       "      <td>mb</td>\n",
       "      <td>21.0</td>\n",
       "      <td>171.0</td>\n",
       "      <td>1.2640</td>\n",
       "      <td>0.56</td>\n",
       "      <td>...</td>\n",
       "      <td>2023-02-23T07:28:35.040Z</td>\n",
       "      <td>106 km WNW of Murghob, Tajikistan</td>\n",
       "      <td>earthquake</td>\n",
       "      <td>8.00</td>\n",
       "      <td>2.000</td>\n",
       "      <td>0.140</td>\n",
       "      <td>15.0</td>\n",
       "      <td>reviewed</td>\n",
       "      <td>us</td>\n",
       "      <td>us</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2023-02-23T06:18:13.280Z</td>\n",
       "      <td>-8.5483</td>\n",
       "      <td>-77.6254</td>\n",
       "      <td>66.540</td>\n",
       "      <td>4.70</td>\n",
       "      <td>mb</td>\n",
       "      <td>44.0</td>\n",
       "      <td>154.0</td>\n",
       "      <td>3.5030</td>\n",
       "      <td>0.47</td>\n",
       "      <td>...</td>\n",
       "      <td>2023-02-23T07:15:40.040Z</td>\n",
       "      <td>22 km SW of Quiches, Peru</td>\n",
       "      <td>earthquake</td>\n",
       "      <td>8.10</td>\n",
       "      <td>8.700</td>\n",
       "      <td>0.064</td>\n",
       "      <td>74.0</td>\n",
       "      <td>reviewed</td>\n",
       "      <td>us</td>\n",
       "      <td>us</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2023-02-23T03:36:09.429Z</td>\n",
       "      <td>-18.2870</td>\n",
       "      <td>-177.8261</td>\n",
       "      <td>524.983</td>\n",
       "      <td>4.90</td>\n",
       "      <td>mb</td>\n",
       "      <td>40.0</td>\n",
       "      <td>63.0</td>\n",
       "      <td>3.6260</td>\n",
       "      <td>0.67</td>\n",
       "      <td>...</td>\n",
       "      <td>2023-02-23T03:50:54.040Z</td>\n",
       "      <td>NaN</td>\n",
       "      <td>earthquake</td>\n",
       "      <td>13.43</td>\n",
       "      <td>10.963</td>\n",
       "      <td>0.108</td>\n",
       "      <td>27.0</td>\n",
       "      <td>reviewed</td>\n",
       "      <td>us</td>\n",
       "      <td>us</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                       time  latitude  longitude    depth   mag magType   nst  \\\n",
       "0  2023-02-23T08:01:02.881Z   38.1280    73.2184   10.000  4.70      mb  75.0   \n",
       "1  2023-02-23T06:55:34.020Z   18.7946   -63.9205   10.000  3.67      md  19.0   \n",
       "2  2023-02-23T06:50:49.137Z   38.4879    72.8122   10.000  4.50      mb  21.0   \n",
       "3  2023-02-23T06:18:13.280Z   -8.5483   -77.6254   66.540  4.70      mb  44.0   \n",
       "4  2023-02-23T03:36:09.429Z  -18.2870  -177.8261  524.983  4.90      mb  40.0   \n",
       "\n",
       "     gap    dmin   rms  ...                   updated  \\\n",
       "0   55.0  1.7450  0.66  ...  2023-02-23T08:35:43.040Z   \n",
       "1  228.0  1.1264  0.50  ...  2023-02-23T08:02:57.172Z   \n",
       "2  171.0  1.2640  0.56  ...  2023-02-23T07:28:35.040Z   \n",
       "3  154.0  3.5030  0.47  ...  2023-02-23T07:15:40.040Z   \n",
       "4   63.0  3.6260  0.67  ...  2023-02-23T03:50:54.040Z   \n",
       "\n",
       "                               place        type horizontalError depthError  \\\n",
       "0     65 km W of Murghob, Tajikistan  earthquake            4.00      1.907   \n",
       "1                    Leeward Islands  earthquake            5.27      4.420   \n",
       "2  106 km WNW of Murghob, Tajikistan  earthquake            8.00      2.000   \n",
       "3          22 km SW of Quiches, Peru  earthquake            8.10      8.700   \n",
       "4                                NaN  earthquake           13.43     10.963   \n",
       "\n",
       "   magError  magNst    status  locationSource magSource  \n",
       "0     0.058    91.0  reviewed              us        us  \n",
       "1     0.130    14.0  reviewed              pr        pr  \n",
       "2     0.140    15.0  reviewed              us        us  \n",
       "3     0.064    74.0  reviewed              us        us  \n",
       "4     0.108    27.0  reviewed              us        us  \n",
       "\n",
       "[5 rows x 22 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "earthquake_df = pd.read_csv(\"data/Latest_earthquake_world.csv\", sep=\",\", header=0, encoding=\"ISO-8859-1\")\n",
    "earthquake_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Count      Mean  Max  Min  Range\n",
      "0  337.0  3.942077  6.8  2.5    4.3\n"
     ]
    }
   ],
   "source": [
    "earthquake_df = pd.read_csv(\"data/Latest_earthquake_world.csv\", sep=\",\", header=0, encoding=\"ISO-8859-1\")\n",
    "earthquake_df.columns = [column.replace(\" \", \"_\") for column in earthquake_df.columns]\n",
    "earth_cnt = earthquake_df[\"mag\"].count()\n",
    "earth_mn = earthquake_df[\"mag\"].mean()\n",
    "earth_max = earthquake_df[\"mag\"].max()\n",
    "earth_min = earthquake_df[\"mag\"].min()\n",
    "earth_rang = earth_max-earth_min\n",
    "earth_stats= pd.Series({\"Count\": earth_cnt, \"Mean\": earth_mn, \"Max\": earth_max, \"Min\": earth_min, \"Range\": earth_rang}).to_frame().transpose()\n",
    "print(earth_stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is also possible to obtain summary statistics for each group separately by applying the very useful *group_by()* method. In the example below, I am obtaining stats for each MagSource and saving them into a DataFrame. The columns do not need to be defined beforehand. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Count</th>\n",
       "      <th>Mean</th>\n",
       "      <th>Max</th>\n",
       "      <th>Min</th>\n",
       "      <th>Range</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>magSource</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>ak</th>\n",
       "      <td>37</td>\n",
       "      <td>3.075676</td>\n",
       "      <td>5.40</td>\n",
       "      <td>2.50</td>\n",
       "      <td>2.90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>av</th>\n",
       "      <td>3</td>\n",
       "      <td>2.836667</td>\n",
       "      <td>2.95</td>\n",
       "      <td>2.63</td>\n",
       "      <td>0.32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ci</th>\n",
       "      <td>2</td>\n",
       "      <td>2.850000</td>\n",
       "      <td>2.90</td>\n",
       "      <td>2.80</td>\n",
       "      <td>0.10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>guc</th>\n",
       "      <td>3</td>\n",
       "      <td>4.600000</td>\n",
       "      <td>4.90</td>\n",
       "      <td>4.40</td>\n",
       "      <td>0.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hv</th>\n",
       "      <td>13</td>\n",
       "      <td>2.753077</td>\n",
       "      <td>3.24</td>\n",
       "      <td>2.54</td>\n",
       "      <td>0.70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nc</th>\n",
       "      <td>10</td>\n",
       "      <td>2.776000</td>\n",
       "      <td>3.22</td>\n",
       "      <td>2.52</td>\n",
       "      <td>0.70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nn</th>\n",
       "      <td>2</td>\n",
       "      <td>3.100000</td>\n",
       "      <td>3.50</td>\n",
       "      <td>2.70</td>\n",
       "      <td>0.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ok</th>\n",
       "      <td>2</td>\n",
       "      <td>2.835000</td>\n",
       "      <td>3.11</td>\n",
       "      <td>2.56</td>\n",
       "      <td>0.55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pr</th>\n",
       "      <td>33</td>\n",
       "      <td>3.076364</td>\n",
       "      <td>3.87</td>\n",
       "      <td>2.53</td>\n",
       "      <td>1.34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>se</th>\n",
       "      <td>2</td>\n",
       "      <td>2.570000</td>\n",
       "      <td>2.61</td>\n",
       "      <td>2.53</td>\n",
       "      <td>0.08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tx</th>\n",
       "      <td>19</td>\n",
       "      <td>3.047368</td>\n",
       "      <td>4.70</td>\n",
       "      <td>2.50</td>\n",
       "      <td>2.20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>us</th>\n",
       "      <td>209</td>\n",
       "      <td>4.500478</td>\n",
       "      <td>6.80</td>\n",
       "      <td>2.60</td>\n",
       "      <td>4.20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>uu</th>\n",
       "      <td>2</td>\n",
       "      <td>3.045000</td>\n",
       "      <td>3.17</td>\n",
       "      <td>2.92</td>\n",
       "      <td>0.25</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           Count      Mean   Max   Min  Range\n",
       "magSource                                    \n",
       "ak            37  3.075676  5.40  2.50   2.90\n",
       "av             3  2.836667  2.95  2.63   0.32\n",
       "ci             2  2.850000  2.90  2.80   0.10\n",
       "guc            3  4.600000  4.90  4.40   0.50\n",
       "hv            13  2.753077  3.24  2.54   0.70\n",
       "nc            10  2.776000  3.22  2.52   0.70\n",
       "nn             2  3.100000  3.50  2.70   0.80\n",
       "ok             2  2.835000  3.11  2.56   0.55\n",
       "pr            33  3.076364  3.87  2.53   1.34\n",
       "se             2  2.570000  2.61  2.53   0.08\n",
       "tx            19  3.047368  4.70  2.50   2.20\n",
       "us           209  4.500478  6.80  2.60   4.20\n",
       "uu             2  3.045000  3.17  2.92   0.25"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "earthquake_df = pd.read_csv(\"data/Latest_earthquake_world.csv\", sep=\",\", header=0, encoding=\"ISO-8859-1\")\n",
    "earthquake_df.columns = [column.replace(\" \", \"_\") for column in earthquake_df.columns]\n",
    "earthquake_stats = pd.DataFrame()\n",
    "earthquake_stats[\"Count\"] = earthquake_df.groupby(\"magSource\")['mag'].count()\n",
    "earthquake_stats[\"Mean\"] = earthquake_df.groupby(\"magSource\")['mag'].mean()\n",
    "earthquake_stats[\"Max\"] = earthquake_df.groupby(\"magSource\")['mag'].max()\n",
    "earthquake_stats[\"Min\"] = earthquake_df.groupby(\"magSource\")['mag'].min()\n",
    "earthquake_stats[\"Range\"] = earthquake_stats[\"Max\"] - earthquake_stats[\"Min\"]\n",
    "earthquake_stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The *describe()* method can be used to obtain a set of default summary statistics for a column of data. Combining this with *group_by()* allows for the calculation of statistics by group. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      count       mean        std        min        25%        50%  \\\n",
      "mag                                                                  \n",
      "2.50   11.0  39.211561  73.200140   2.000000   7.117415  11.200000   \n",
      "2.52    1.0   4.410000        NaN   4.410000   4.410000   4.410000   \n",
      "2.53    2.0   7.890000   6.420530   3.350000   5.620000   7.890000   \n",
      "2.54    1.0  30.959999        NaN  30.959999  30.959999  30.959999   \n",
      "2.54    2.0  11.120000   0.579828  10.710000  10.915000  11.120000   \n",
      "...     ...        ...        ...        ...        ...        ...   \n",
      "5.60    2.0  80.423000  62.782597  36.029000  58.226000  80.423000   \n",
      "5.70    1.0  20.395000        NaN  20.395000  20.395000  20.395000   \n",
      "6.10    1.0  38.615000        NaN  38.615000  38.615000  38.615000   \n",
      "6.30    1.0  16.000000        NaN  16.000000  16.000000  16.000000   \n",
      "6.80    1.0  20.522000        NaN  20.522000  20.522000  20.522000   \n",
      "\n",
      "             75%         max  \n",
      "mag                           \n",
      "2.50   25.250000  250.000000  \n",
      "2.52    4.410000    4.410000  \n",
      "2.53   10.160000   12.430000  \n",
      "2.54   30.959999   30.959999  \n",
      "2.54   11.325000   11.530000  \n",
      "...          ...         ...  \n",
      "5.60  102.620000  124.817000  \n",
      "5.70   20.395000   20.395000  \n",
      "6.10   38.615000   38.615000  \n",
      "6.30   16.000000   16.000000  \n",
      "6.80   20.522000   20.522000  \n",
      "\n",
      "[79 rows x 8 columns]\n"
     ]
    }
   ],
   "source": [
    "print(earthquake_df.groupby(\"mag\")[\"depth\"].describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Concatenate and Merge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Pandas *concat()* method is used to **concatenate** DataFrames that have the same columns. This is comparable to copying and pasting rows from two spreadsheets into a new spreadsheet. To demonstrate this, I have extracted rows using indexes. Next, I concatenate them back to a new DataFrame. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "337\n",
      "237\n"
     ]
    }
   ],
   "source": [
    "earthquake_df = pd.read_csv(\"data/Latest_earthquake_world.csv\", sep=\",\", header=0, encoding=\"ISO-8859-1\")\n",
    "earthquake_df.columns =[column.replace(\" \", \"_\") for column in earthquake_df.columns]\n",
    "\n",
    "earth_sub1 = earthquake_df[100:500]\n",
    "earth_sub2 = earthquake_df[900:1300]\n",
    "earth_subc = pd.concat([earth_sub1, earth_sub2])\n",
    "print(len(earthquake_df))\n",
    "print(len(earth_subc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Merge** is comparable to table joins when using SQL. This requires the use of **keys** and the declaration of a joining method, such as \"Left\", \"Right\", \"Inner\", or \"Outer\". \n",
    "\n",
    "In the example, I first create a unique ID by copying the row index to a column (idx).\n",
    "\n",
    "I then break the data into two components, each containing the ID and a subset of the remaining columns. \n",
    "\n",
    "I then use the *merge()* method to merge the DataFrames using the \"inner\" method and the common \"id\" field. \"Inner\" will only return rows that occur in both data sets. Since both DataFrames were derived from the same original DataFrame, they will have identical rows, so the result would be the same as using \"left\", where all rows from the left table are maintained even if they don't occur in the right table, or \"right\", where all rows from the right table are maintained even if they don't occur in the left table. \n",
    "\n",
    "In the second example, I use a query to separate out only earthquakes with magnitude of more than 4.0. When I perform a join with all of the data using the \"inner\" method, I only get back the common or shared rows. \n",
    "\n",
    "Note that there is also a *join()* method that joins based on indexes. However, that will not be demonstrated here. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                       time  latitude  longitude  depth   mag magType   nst  \\\n",
      "0  2023-02-23T08:01:02.881Z   38.1280    73.2184   10.0  4.70      mb  75.0   \n",
      "1  2023-02-23T06:55:34.020Z   18.7946   -63.9205   10.0  3.67      md  19.0   \n",
      "\n",
      "     gap    dmin   rms  ...                           place        type  \\\n",
      "0   55.0  1.7450  0.66  ...  65 km W of Murghob, Tajikistan  earthquake   \n",
      "1  228.0  1.1264  0.50  ...                 Leeward Islands  earthquake   \n",
      "\n",
      "  horizontalError depthError magError  magNst    status  locationSource  \\\n",
      "0            4.00      1.907    0.058    91.0  reviewed              us   \n",
      "1            5.27      4.420    0.130    14.0  reviewed              pr   \n",
      "\n",
      "   magSource idx  \n",
      "0         us   0  \n",
      "1         pr   1  \n",
      "\n",
      "[2 rows x 23 columns]\n",
      "\n",
      "337\n",
      "\n",
      "   idx   mag    depth                              place\n",
      "0    0  4.70   10.000     65 km W of Murghob, Tajikistan\n",
      "1    1  3.67   10.000                    Leeward Islands\n",
      "2    2  4.50   10.000  106 km WNW of Murghob, Tajikistan\n",
      "3    3  4.70   66.540          22 km SW of Quiches, Peru\n",
      "4    4  4.90  524.983                                NaN\n",
      "\n",
      "337\n",
      "\n",
      "   idx  mag                              place    depth\n",
      "0    0  4.7     65 km W of Murghob, Tajikistan   10.000\n",
      "1    2  4.5  106 km WNW of Murghob, Tajikistan   10.000\n",
      "2    3  4.7          22 km SW of Quiches, Peru   66.540\n",
      "3    4  4.9                                NaN  524.983\n",
      "4    5  4.4         52 km WNW of Ovalle, Chile   34.097\n",
      "\n",
      "192\n",
      "\n"
     ]
    }
   ],
   "source": [
    "earthquake_df = pd.read_csv(\"data/Latest_earthquake_world.csv\", sep=\",\", header=0, encoding=\"ISO-8859-1\")\n",
    "\n",
    "earthquake_df[\"idx\"] = earthquake_df.index\n",
    "print(earthquake_df.head(2))\n",
    "print('')\n",
    "print(len(earthquake_df))\n",
    "print('')\n",
    "subset_first = earthquake_df[[\"idx\", \"mag\"]]\n",
    "subset_second = earthquake_df[[\"idx\", \"depth\", \"place\"]]\n",
    "\n",
    "earth_merge = pd.merge(subset_first, subset_second, how=\"inner\", on=\"idx\")\n",
    "print(earth_merge.head(5))\n",
    "print('')\n",
    "print(len(earth_merge))\n",
    "print('')\n",
    "subset_third = earthquake_df.query('mag > 4')[[\"idx\", \"place\", \"depth\"]]\n",
    "earth_merge2 = pd.merge(subset_first, subset_third, how=\"inner\", on=\"idx\")\n",
    "print(earth_merge2.head(5))\n",
    "print('')\n",
    "print(len(earth_merge2))\n",
    "print('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final remarks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well there is much to discuss and learn from the use of Pandas, but this is the initial step to get you familiarize with this library. Now it is your turn to try out the exercises to help you to recall and apply all the methods in these two notebook. So please open the **Exercises_NumPy_Pandas.ipynb** work on it. \n",
    "\n",
    "For more examples and details, please consult the documentation for [Pandas](https://pandas.pydata.org/docs/reference/io.html). \n",
    "\n",
    "In the next week, we will discuss methods for graphing and visualizing data using **matplotlib**, and **Pandas**. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "vscode": {
   "interpreter": {
    "hash": "a62a218f45948969006c944db2ca1c519af623da5e08f864ae6aafcacb945df1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
